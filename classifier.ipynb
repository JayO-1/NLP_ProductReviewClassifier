{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc3e103-c05c-4d8c-8b47-30989e3c0ce1",
   "metadata": {},
   "source": [
    "# Product Review Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "The neural network I decided to use was a Bidirectional LSTM, as I believed that for this particular use case, where the ordering of the words in each review would influence subsequent semantic meaning, I could achieve a higher accuracy with a model that was able to analyse text from both left to right and right to left. This involved an embedding layer, followed by the Bidirectional LSTM followed by a sigmoid layer for classification, with binary cross entropy loss as my loss function.\n",
    "\n",
    "I first determined a way of parsing the data within the corpus to construct a training set. I did not believe that feeding entire reviews, which are made up of multiple sentences, directly into my model would prove successful as this would mean the input sequences would be very large. This is a problem, as:\n",
    "\n",
    "1. With longer sequences, even though LSTMs help to mitigate this, we can begin to encounter the\n",
    "vanishing gradient problem\n",
    "2. Longer sequences result in a Neural Network that is quite large, leading to longer training times\n",
    "\n",
    "In addition to the above, reviews can be difficult to assign to a single label as they will often have both positive and negative elements to them as well as unannotated sentences, leading to ambiguity with regards to how best to classify them.\n",
    "\n",
    "Thus, to circumnavigate these issues, I decided to process the corpus on a sentence-by-sentence basis. Each annotated sentence would be given a label of 1 if the opinion was positive and 0 if the opinion was negative (which is why I use a sigmoid layer for classification). The idea here is that if one wanted to use this system to classify longer articles, they could simply aggregate the scores for each sentence, then threshold the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d1d1e-0a0a-46bc-97b2-03e876b900bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string, re, random, statistics, math\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc2fe9-ebb4-41bb-a9dd-7ac66939e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(corpus_root: str, file_pattern: str):\n",
    "    \"\"\"\n",
    "    Read files from a directory and then append the raw data of each file onto a list\n",
    "    \"\"\"\n",
    "    wordlists = PlaintextCorpusReader(corpus_root, file_pattern)\n",
    "    fileids = wordlists.fileids()\n",
    "\n",
    "    data = []\n",
    "    valid_fileids = []\n",
    "\n",
    "    # Constructing list of tuples in the form (docid, raw doc contents)\n",
    "    for f_id in fileids:\n",
    "        if f_id != \"README.txt\":\n",
    "            data.append((f_id, wordlists.raw(f_id)))\n",
    "            valid_fileids.append(f_id)\n",
    "    \n",
    "    return data, valid_fileids\n",
    "\n",
    "def clean(sentence: str):\n",
    "    \"\"\"\n",
    "    Clean the input string\n",
    "    \"\"\"\n",
    "    # Tokenize the document\n",
    "    sentence_to_add = word_tokenize(sentence)\n",
    "\n",
    "    # Then, casefolding the document by lowercasing everything\n",
    "    sentence_to_add = [word.lower() for word in sentence_to_add]\n",
    "\n",
    "    # Next, removing stop words\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    sentence_to_add = [w for w in sentence_to_add if w not in stopwords]\n",
    "\n",
    "    # In addition, removing punctuation\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    sentence_to_add = [w.translate(punc_table) for w in sentence_to_add]\n",
    "\n",
    "    # Above lines replace punctuation tokens with whitespace, so must\n",
    "    # filter them out of the wordlist\n",
    "    while(\"\" in sentence_to_add):\n",
    "        sentence_to_add.remove(\"\")\n",
    "    \n",
    "    # Removing numbers, as they do not convey meaning\n",
    "    pattern = \"\\d+\"\n",
    "    sentence_to_add = [w for w in sentence_to_add if not re.match(pattern, w)]\n",
    "    \n",
    "    return \" \".join(sentence_to_add)\n",
    "    \n",
    "def preprocess(data: list):\n",
    "    \"\"\"\n",
    "    pre-process and construct the training set for the corpus\n",
    "    str->list\"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    vocabulary = set()\n",
    "    for docid, text in data:\n",
    "        # First split on [t] to extract reviews for doc\n",
    "        reviews = re.split('\\[t\\]', text)\n",
    "        \n",
    "        # Next, process each review\n",
    "        for review in reviews:\n",
    "            # Split on newline char to retrieve sentences\n",
    "            sentences = review.split('\\n')\n",
    "            \n",
    "            # For each sentence, store class in Y if the information is present\n",
    "            # i.e. [+1] or [-3] otherwise ignore it\n",
    "            # 1 -> Positive Class\n",
    "            # 0 -> Negative Class\n",
    "            for sentence_num in range(len(sentences)):\n",
    "                sentence = sentences[sentence_num]\n",
    "                \n",
    "                # By splitting on ##, we can divide the sentence into its label and its\n",
    "                # content\n",
    "                label_and_content = sentence.split('##')\n",
    "                \n",
    "                # Then we must decide what class to assign each sentence\n",
    "                if len(label_and_content) == 2:\n",
    "                    label = label_and_content[0]\n",
    "                    content = label_and_content[1]\n",
    "                    \n",
    "                    # Depending on + or -, we also store an appropriate label\n",
    "                    # If both are present, the label is determined by the sum of the\n",
    "                    # opinion values\n",
    "                    if '+' in label and '-' in label:\n",
    "                        opinions = re.findall('[+|-]\\d', label)\n",
    "                        net_opinion = 0\n",
    "                        for opinion in opinions:\n",
    "                            value = int(opinion)\n",
    "                            net_opinion += int(value)\n",
    "                        \n",
    "                        if net_opinion > 0:\n",
    "                            Y.append(1)\n",
    "                        else:\n",
    "                            Y.append(0)\n",
    "                    elif '+' in label:\n",
    "                        Y.append(1)\n",
    "                    elif '-' in label:\n",
    "                        Y.append(0)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the sentence then store\n",
    "                    sentence_to_add = clean(content)\n",
    "                    X.append(sentence_to_add)\n",
    "                    \n",
    "                    # We also maintain a vocabulary of the words in the corpus\n",
    "                    sentence_to_add = sentence_to_add.split(\" \")\n",
    "                    vocabulary.update(sentence_to_add)\n",
    "    \n",
    "    # Remove empty tokens left behind after processing\n",
    "    vocabulary.remove('')\n",
    "    \n",
    "    while '' in X:\n",
    "        ind_of_empty = X.index('')\n",
    "        X.remove('')\n",
    "        Y.pop(ind_of_empty)\n",
    "    \n",
    "    # Convert vocab back into a list for sentence to sequence conversion\n",
    "    # and log the vocab size\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    # Now that the labelled sentences in the corpus have been cleaned and stored, we must now\n",
    "    # vectorize each sentence\n",
    "    # This is done by replacing the word in each sentence with its id in the vocabulary\n",
    "    for i in range(len(X)):\n",
    "        review_sentence = X[i]\n",
    "        text_as_sequence = []\n",
    "        tokens = review_sentence.split(\" \")\n",
    "        for token in tokens:\n",
    "            text_as_sequence.append(vocabulary.index(token))\n",
    "        \n",
    "        X[i] = text_as_sequence\n",
    "    \n",
    "    # Finally, pad the data to ensure uniform dimensions\n",
    "    # Data matrix is padded to the length of the longest sentence\n",
    "    max_length = 0\n",
    "    for example in X:\n",
    "        max_length = max(max_length, len(example))\n",
    "    \n",
    "    padding_type = 'post'\n",
    "    \n",
    "    X_padded = pad_sequences(X, maxlen=max_length, padding=padding_type)\n",
    "    \n",
    "    return np.array(X_padded), np.array(Y), vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ef7c9-a822-48d2-a099-182247b7902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus root = product_review location - set appropriately\n",
    "corpus_root = r\"C:\\Users\\halor\\Desktop\\NLP_ProductReviewClassifier\\product_reviews\"\n",
    "file_pattern = r\".*.txt\"\n",
    "\n",
    "data, fileids = read_data(corpus_root, file_pattern)\n",
    "X, Y, vocab_size = preprocess(data)\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422dc60-8f9a-4784-a7aa-0dda3ab11b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = compile_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "To examine the modelâ€™s performance, I used K-Fold Cross Validation. Analysing the results, I determined that with appropriate hyperparameter selection the model performs well considering the small amount of training data (approx. 2k sentences), achieving an average accuracy of around 72% across 5 folds. The hyperparameters tuned in this case were the learning rate and the number of epochs. Optimal results were achieved with a learning rate of 0.0001 and 13 to 15 epochs.\n",
    "\n",
    "As previously mentioned, I believe the success of this model can be attributed to the fact that it is able to learn the semantic relationship between the tokens in each sentence not just from left to right but also from right to left, improving accuracy. This is confirmed by the fact that with the same experimentation on a normal LSTM, I was only able to achieve an average accuracy of around 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75955b-2e2f-44d2-923a-b3cf55a45601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold validation, with k = 5\n",
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# The results across all iterations of KFold are compiled to determine the average accuracy\n",
    "# and the standard deviation\n",
    "results = []\n",
    "count = 0\n",
    "num_epochs = 13\n",
    "for train, test in kfold.split(X, Y):\n",
    "    trainX, testX = X[train], X[test]\n",
    "    trainY, testY = Y[train], Y[test]\n",
    "    \n",
    "    # Recompiling the model to reset the weights, then training on the new split\n",
    "    model = compile_model()\n",
    "    history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=num_epochs, batch_size=64)\n",
    "    \n",
    "    try:\n",
    "        accuracy = history.history['val_accuracy'][num_epochs - 1]\n",
    "    except:\n",
    "        accuracy = history.history['val_acc'][num_epochs - 1]\n",
    "        \n",
    "    print('Iteration: #{} Accuracy: {}%'.format(count + 1, accuracy * 100))\n",
    "    results.append(accuracy)\n",
    "    count += 1\n",
    "\n",
    "print('\\nAverage accuracy: {}%'.format(statistics.mean(results) * 100))\n",
    "print('Standard deviation: {}'.format(statistics.stdev(results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
